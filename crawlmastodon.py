# -*- coding: utf-8 -*-
"""crawlMastodon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/121OUKJx9fU_YJc57ZAnLl5uoC_I-ANx6
"""

pip install Mastodon.py beautifulsoup4 pandas

from mastodon import Mastodon
from bs4 import BeautifulSoup
import pandas as pd
import time
import os

# Configuration
INSTANCE = "https://social.cologne"  # You can change to a German instance like chaos.social or queer.party
HASHTAGS = ["queer", "lgbtq", "trans", "lesbisch", "schwul", "bisexuell",
    "pansexuell", "nicht-binär", "genderqueer", "asexuell", "inter*",
    "polyamor", "genderfluid", "demisexuell", "agender", "transfrau","transmann"]
LIMIT = 100  # Number of toots per hashtag (max 100 per request)
LANGUAGE = "de"  # ISO code for German
DELAY = 1  # seconds between requests to avoid rate limiting

# Authenticate (public app only, no login required)
mastodon = Mastodon(
    api_base_url=INSTANCE
)

def clean_html(raw_html):
    return BeautifulSoup(raw_html, "html.parser").get_text()

# Crawl
all_toots = []
for tag in HASHTAGS:
    print(f"⏳ Fetching #{tag}...")
    try:
        toots = mastodon.timeline_hashtag(tag, limit=LIMIT)
        german_toots = [toot for toot in toots if toot.get('language') == 'de']
        for toot in german_toots:
            all_toots.append({
                'username': toot['account']['acct'],
                'created_at': toot['created_at'],
                'tag': tag,
                'content_raw': toot['content'],
                'content_clean': clean_html(toot['content'])
            })
        time.sleep(DELAY)
    except Exception as e:
        print(f"⚠️ Error fetching #{tag}: {e}")

# Save
df = pd.DataFrame(all_toots)
file_path = "mastodon_queer_german_posts.csv"
if os.path.exists(file_path):
    existing_df = pd.read_csv(file_path)
    # Append the new data to the existing DataFrame
    updated_df = pd.concat([existing_df, df], ignore_index=True)
else:
    updated_df = df

# Save the combined DataFrame back to the CSV file
updated_df.to_csv(file_path, index=False)

print(f"✅ Appended {len(df)} posts to {file_path}. Total posts: {len(updated_df)}")

# prompt: filter out entries which contain "#GutenMorgen liebe #Mastodonier und #Fediverse" and "#nsfw" in content_clean

# Filter out rows based on conditions in 'content_clean'
# Keep rows where 'content_clean' does NOT contain "#GutenMorgen liebe #Mastodonier und #Fediverse"
# AND 'content_clean' does NOT contain "#nsfw"
updated_df = updated_df[
    (~updated_df['content_clean'].str.contains("#GutenMorgen liebe #Mastodonier und #Fediverse", na=False)) &
     (~updated_df['content_clean'].str.contains("Ihr Lieben auf https:", na=False)) &
    (~updated_df['content_clean'].str.contains("#nsfw", na=False))
]

print(f"✅ Filtered dataframe. Remaining posts: {len(updated_df)}")
file_path = "mastodon_queer_german_posts.csv"
updated_df.to_csv(file_path, index=False)

import pandas as pd
import re

# Define regex patterns
hashtag_pattern = re.compile(r"#\w+")
url_pattern = re.compile(r"http\S+|www\S+")

# Hashtags to optionally retain
KEEP_HASHTAGS = {
   "queer", "lgbtq", "trans", "lesbisch", "schwul", "bisexuell",
    "pansexuell", "nicht-binär", "genderqueer", "asexuell", "inter*",
    "polyamor", "genderfluid", "demisexuell", "agender", "transfrau","transmann"
}

def clean_text(text):
    text = str(text)

    # Remove URLs
    text = re.sub(url_pattern, '', text)

    # Find hashtags and remove them
    hashtags_found = set(re.findall(hashtag_pattern, text.lower()))
    text_no_hashtags = re.sub(hashtag_pattern, '', text)

    # Re-add kept hashtags (optional)
    retained = hashtags_found & KEEP_HASHTAGS
    if retained:
        text_no_hashtags = text_no_hashtags.strip() + " " + " ".join(retained)

    # Remove excess whitespace
    return re.sub(r'\s+', ' ', text_no_hashtags).strip()

# Apply cleaning
updated_df['text'] = updated_df['content_clean'].astype(str).apply(clean_text)

# Save to plain text for fine-tuning
with open("queer_posts_finetune.txt", "w", encoding="utf-8") as f:
    for row in updated_df['text']:
        f.write(row + "\n\n")

print("✅ Cleaned text saved to: queer_posts_finetune.txt")